{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torch import tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shrijansshetty/dev/GeneratedImageDetector/ResNet50'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_data/a6dcb93f596a43249135678dfcfc17ea.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_data/041be3153810433ab146bc97d5af505c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_data/8542fe161d9147be8e835e50c0de39cd.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name  label\n",
       "0  train_data/a6dcb93f596a43249135678dfcfc17ea.jpg      1\n",
       "1  train_data/041be3153810433ab146bc97d5af505c.jpg      0\n",
       "2  train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg      1\n",
       "3  train_data/8542fe161d9147be8e835e50c0de39cd.jpg      0\n",
       "4  train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the root directory of the dataset\n",
    "dataset_root = '../Dataset'\n",
    "# Load the train CSV file\n",
    "train_df = pd.read_csv(os.path.join(dataset_root, 'train.csv'), index_col=0)\n",
    "# Load the test CSV file\n",
    "test_df = pd.read_csv(os.path.join(dataset_root, 'test.csv'))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 63960, Validation size: 15990\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test  # Flag to indicate if this is the test dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            # Use the first column (assumed to be 'id' or 'file_name')\n",
    "            img_path = os.path.join(self.root_dir, self.df.iloc[idx, 0])  \n",
    "        else:\n",
    "            # Use column names instead of hardcoded index\n",
    "            img_path = os.path.join(self.root_dir, self.df['file_name'].iloc[idx])  \n",
    "            label = int(self.df['label'].iloc[idx])  \n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, -1 \n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Transform (with data augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomHorizontalFlip(),  # Augmentation: Randomly flip images\n",
    "    # transforms.RandomRotation(10),      # Augmentation: Rotate images slightly\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Test/Validation Transform (NO augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Keep it consistent\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 63960\n",
      "Validation dataset size: 15990\n",
      "Test dataset size: 5540\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageDataset(train_df, dataset_root, transform=train_transform, is_test=False)\n",
    "val_dataset = ImageDataset(val_df, dataset_root, transform=test_transform, is_test=False)\n",
    "test_dataset = ImageDataset(test_df, dataset_root, transform=test_transform, is_test=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(val_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/h3j36kwj6qb9gp2_c6z48bbr0000gn/T/ipykernel_2528/3551722438.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load ResNet50 model with pre-trained weights\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Modify the fully connected (fc) layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "for param in model.layer4.parameters():  # Unfreeze the last ResNet block\n",
    "    param.requires_grad = True  \n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True  # Keep final layer trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "#AdamW helps improve generalization.\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR by 10x every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0119, Training Accuracy: 99.58%, Validation Accuracy: 99.30%\n",
      "Learning Rate: 0.0005\n",
      "Epoch 2/20, Loss: 0.0062, Training Accuracy: 99.79%, Validation Accuracy: 99.56%\n",
      "Learning Rate: 0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Lab2/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20  # Increased to give early stopping a chance to work\n",
    "patience = 3  # Number of epochs to wait before stopping if no improvement\n",
    "best_val_acc = 0  # Track the best validation accuracy\n",
    "counter = 0  # Counter for patience\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]}')  # Print current learning rate\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early Stopping Logic\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        counter = 0  # Reset patience counter\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break  # Stop training if no improvement for `patience` epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:  # No labels in the test set\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Ensure IDs are correctly extracted from test_df\n",
    "submission_df = pd.DataFrame({'id': test_df.iloc[:, 0], 'label': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  id  label\n",
      "0  test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg      0\n",
      "1  test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg      0\n",
      "2  test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg      0\n",
      "3  test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg      0\n",
      "4  test_data_v2/a16495c578b7494683805484ca27cf9f.jpg      0\n"
     ]
    }
   ],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Check the first few rows\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9902\n",
      "Recall: 0.9967\n",
      "F1 Score: 0.9935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Collect predictions and labels\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:  # Use val_loader or test_loader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(model, image_tensor, class_names):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class index with the highest score\n",
    "        return class_names[predicted.item()]\n",
    "# Step 3: Preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "        transforms.ToTensor(),         # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Real\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved weights and input image\n",
    "weights_path = \"best_model.pth\"\n",
    "image_path = \"/Users/shrijansshetty/dev/GeneratedImageDetector/Dataset/train_data/ffd74e6a0c314e969df97f49dfcb8b6f.jpg\"\n",
    "class_names = [\"Real\", \"Fake\"]  # Replace with your actual class names\n",
    "num_classes = 2  # Number of classes in your dataset\n",
    "\n",
    "# Load the model and preprocess the image\n",
    "image_tensor = preprocess_image(image_path)\n",
    "\n",
    "# Classify the image\n",
    "result = classify_image(model, image_tensor, class_names)\n",
    "print(f\"Predicted Class: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrijansshetty/miniconda3/envs/Lab2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shrijansshetty/miniconda3/envs/Lab2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/0z/h3j36kwj6qb9gp2_c6z48bbr0000gn/T/ipykernel_2528/2533870059.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the trained model\n",
    "# Replace 'best_model.pth' with the path to your trained model weights\n",
    "# Load ResNet50 model with pre-trained weights\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Modify the fully connected (fc) layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Access the first convolutional layer\n",
    "# For ResNet50, the first convolutional layer is `conv1`\n",
    "conv1_weights = model.conv1.weight.data.cpu().numpy()  # Shape: (64, 3, 7, 7)\n",
    "\n",
    "# Step 3: Normalize the weights for visualization\n",
    "def normalize_kernel(kernel):\n",
    "    kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())  # Normalize to [0, 1]\n",
    "    return kernel\n",
    "\n",
    "# Step 4: Visualize the kernels\n",
    "def visualize_kernels(kernels, num_kernels=16):\n",
    "    kernels = kernels[:num_kernels]  # Select the first `num_kernels` filters\n",
    "    num_cols = 4  # Number of columns in the plot\n",
    "    num_rows = (num_kernels + num_cols - 1) // num_cols  # Calculate rows dynamically\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(kernels):\n",
    "            kernel = kernels[i]\n",
    "            kernel = normalize_kernel(kernel)  # Normalize the kernel\n",
    "            # For RGB kernels, transpose to (H, W, C) for visualization\n",
    "            if kernel.shape[0] == 3:  # RGB kernel\n",
    "                kernel = np.transpose(kernel, (1, 2, 0))\n",
    "            ax.imshow(kernel, cmap=\"viridis\" if kernel.shape[-1] != 3 else None)\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")  # Hide unused subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 5: Call the visualization function\n",
    "visualize_kernels(conv1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
